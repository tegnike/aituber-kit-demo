{
  "Description": "■ À propos de l'application",
  "BasedSettings": "■ Paramètres de base",
  "AISettings": "■ Paramètres AI",
  "CharacterSettings": "■ Paramètres de personnage",
  "YoutubeSettings": "■ Paramètres YouTube",
  "VoiceSettings": "■ Paramètres de voix",
  "SlideSettings": "■ Paramètres de diapositive",
  "LogSettings": "■ Historique des conversations",
  "OtherSettings": "■ Autre",
  "ExternalLinkageMode": "Mode de liaison externe (WebSocket)",
  "YoutubeMode": "Mode YouTube",
  "YoutubeInfo": "Le premier caractère du commentaire est '#' et sera ignoré.",
  "YoutubeAPIKey": "Clé API YouTube",
  "YoutubeLiveID": "ID en direct YouTube",
  "ConversationContinuityMode": "Mode de continuité de conversation (Beta)",
  "ConversationContinuityModeInfo": "Quand il n'y a pas de commentaire, l'IA essaie de continuer la conversation. Actuellement uniquement OpenAI, Anthropic Claude, Google Gemini sont supportés.",
  "ConversationContinuityModeInfo2": "Une réponse appelle LLM plusieurs fois, donc l'utilisation de l'API peut augmenter. Veuillez en tenir compte.",
  "ConversationContinuityModeInfo3": "gpt-4, gpt-4-turbo, claude-3-opus, claude-3.5-sonnet fonctionnent relativement stablement.",
  "MaxPastMessages": "Nombre de messages passés à conserver",
  "StatusOn": "Statut : ACTIVÉ",
  "StatusOff": "Statut : DÉSACTIVÉ",
  "Select": "Sélectionner",
  "TestVoice": "Tester la voix",
  "SelectAIService": "Sélectionner le service IA",
  "LocalLLM": "LLM local",
  "SelectModel": "Sélectionner le modèle",
  "OpenAIAPIKeyLabel": "Clé API OpenAI",
  "AnthropicAPIKeyLabel": "Clé API Anthropic",
  "GoogleAPIKeyLabel": "Clé API Google Gemini",
  "AzureAPIKeyLabel": "Clé API Azure OpenAI",
  "AzureAPIURL": "URL API Azure OpenAI",
  "GroqAPIKeyLabel": "Clé API Groq",
  "CohereAPIKeyLabel": "Clé API Cohere",
  "MistralAIAPIKeyLabel": "Clé API MistralAI",
  "PerplexityAPIKeyLabel": "Clé API Perplexity",
  "FireworksAPIKeyLabel": "Clé API Fireworks",
  "DifyAPIKeyLabel": "Clé API Dify",
  "DeepSeekAPIKeyLabel": "Clé API DeepSeek",
  "APIKeyInstruction": "Vous pouvez obtenir la clé API ci-dessous. Entrez la clé API obtenue dans le formulaire.",
  "LocalLLMInfo": "Le serveur LLM local doit être en cours d'exécution. La configuration est la suivante.",
  "LocalLLMInfo2": "Veuillez entrer l'URL du serveur LLM local (y compris le numéro de port) et le nom du modèle.",
  "GroqInfo": "L'API Groq est accessible directement depuis le navigateur.",
  "DifyInfo": "Dify ne prend en charge que les types chatbot et agent.",
  "DifyInfo2": "La longueur de l'historique des conversations dépend des spécifications de Dify.",
  "DifyInfo3": "Exemple : https://api.dify.ai/v1, http://localhost:80/v1",
  "DifyInstruction": "Si vous utilisez Dify, l'invite système ne sera pas utilisée. Veuillez configurer le chatbot Dify.",
  "EnterURL": "URL",
  "CharacterModelLabel": "Modèle de personnage",
  "CharacterModelInfo": "Le modèle peut prendre du temps à charger lors du premier affichage.",
  "OpenVRM": "Ouvrir VRM",
  "BackgroundImage": "Image de fond",
  "ChangeBackgroundImage": "Changer l'image de fond",
  "CharacterSettingsPrompt": "Invite de personnage",
  "CharacterSettingsInfo": "Cette valeur est définie comme l'invite système.\nVeuillez vous référer à l'invite initiale et spécifier les balises d'émotion pour contrôler les expressions et les mouvements du personnage. Exemple : [neutral]Bonjour![happy]Aujourd'hui est aussi une journée difficile!",
  "CharacterSettingsReset": "Réinitialiser les paramètres du personnage",
  "SyntheticVoiceEngineChoice": "Choisir le moteur de synthèse vocale",
  "VoiceAdjustment": "Ajustement de la voix",
  "VoiceEngineInstruction": "Sélectionnez le moteur de synthèse vocale que vous souhaitez utiliser.",
  "UsingKoeiromap": "Koeiromap",
  "KoeiromapInfo": "Utilisation de l'API Koeiromap de Koemotion. Ne prend en charge que le japonais. Pour plus de détails, veuillez consulter le lien ci-dessous.",
  "UsingVoiceVox": "VOICEVOX",
  "VoiceVoxInfo": "Utilisation de VOICEVOX. Ne prend en charge que le japonais. Utilise une API locale, vous devez télécharger et lancer l'application qui convient à votre environnement depuis le site ci-dessous.",
  "VoicevoxSpeed": "Vitesse",
  "VoicevoxPitch": "Hauteur",
  "VoicevoxIntonation": "Intonation",
  "UsingAivisSpeech": "AivisSpeech",
  "AivisSpeechInfo": "Utilisation d'AivisSpeech. Ne prend en charge que le japonais. Utilise une API locale, vous devez télécharger et lancer l'application qui convient à votre environnement depuis le site ci-dessous.",
  "AivisSpeechSpeaker": "Locuteur",
  "AivisSpeechSpeed": "Vitesse",
  "AivisSpeechPitch": "Hauteur",
  "AivisSpeechIntonation": "Intonation",
  "AivisSpeechServerUrl": "URL du serveur AivisSpeech",
  "UsingNijiVoice": "NijiVoice",
  "NijiVoiceInfo": "L'API NijiVoice est utilisée. Ne prend en charge que le japonais. La clé API peut être obtenue depuis l'URL ci-dessous.",
  "NijiVoiceApiKey": "Clé API NijiVoice",
  "NijiVoiceActorId": "ID de l'acteur",
  "NijiVoiceSpeed": "Vitesse de parole",
  "NijiVoiceEmotionalLevel": "Niveau émotionnel",
  "NijiVoiceSoundDuration": "Durée du son",
  "VoicevoxServerUrl": "URL du serveur VOICEVOX",
  "UpdateSpeakerList": "Mettre à jour la liste des locuteurs",
  "UsingGoogleTTS": "Google TTS",
  "UsingStyleBertVITS2": "Style-Bert-VITS2",
  "StyleBertVITS2Info": "Utilisation de Style-Bert-VITS2. Ne prend en charge que le japonais, l'anglais et le chinois. Si vous utilisez une API locale, vous devez télécharger et lancer l'application qui convient à votre environnement depuis le site ci-dessous. Veuillez également configurer une clé API si nécessaire.",
  "SpeakerSelection": "Sélection du locuteur",
  "IncludeTimestampInUserMessage": "Inclure l'horodatage dans le message utilisateur",
  "IncludeTimestampInUserMessageInfo": "En incluant des horodatages dans les messages utilisateur, l'IA peut générer des réponses en tenant compte du temps.\nVeuillez inclure le texte suivant dans votre invite système :\n\n\"Les entrées utilisateur peuvent inclure [timestamp]. Cela représente l'heure UTC au moment de la requête, veuillez donc générer des réponses en tenant compte de cet horodatage.\"",
  "GoogleTTSInfo": "Utilisation de Google Cloud Text-to-Speech. Prend en charge plusieurs langues.",
  "AuthFileInstruction": "Une clé API ou un fichier d'authentification est requis. Obtenez-le depuis l'URL ci-dessous et placez-le dans le dossier racine du dépôt s'il s'agit d'un fichier JSON.",
  "LanguageModelURL": "Sélectionnez le modèle de langue depuis l'URL ci-dessous.",
  "LanguageChoice": "Choix de la langue",
  "StyleBeatVITS2ServerURL": "URL du serveur",
  "StyleBeatVITS2ApiKey": "Clé API",
  "StyleBeatVITS2ModelID": "ID du modèle",
  "StyleBeatVITS2Style": "Style",
  "StyleBeatVITS2SdpRatio": "Ratio de mixage SDP/DP",
  "StyleBeatVITS2Length": "Débit de parole",
  "ConversationHistory": "Historique des conversations",
  "ConversationHistoryInfo": "Les 10 derniers textes de conversation sont stockés comme mémoires.",
  "ConversationHistoryReset": "Réinitialiser l'historique des conversations",
  "NotConnectedToExternalAssistant": "Non connecté à un assistant externe.",
  "APIKeyNotEntered": "La clé API n'est pas saisie.",
  "ChatLog": "Journal des conversations",
  "EnterYourQuestion": "Entrez votre question ici",
  "AnswerGenerating": "Génération de la réponse",
  "AboutThisApplication": "À propos de cette application",
  "AboutThisApplicationDescription": "Profitez de conversations avec un personnage 3D directement dans votre navigateur web, en utilisant le microphone ou la saisie de texte et la synthèse vocale. Vous pouvez également changer le personnage (VRM), ajuster sa personnalité et modifier sa voix.<br />Les paramètres peuvent être modifiés depuis le bouton menu en haut à gauche.",
  "AboutThisApplicationDescription2": "Si vous souhaitez changer le personnage, veuillez consulter l'onglet \"Paramètres de personnage\".",
  "TechnologyIntroduction": "Introduction à la technologie",
  "TechnologyIntroductionDescription1": "Cette application a été créée en modifiant le <b>ChatVRM</b> de pixiv. Le code source original peut être trouvé",
  "TechnologyIntroductionLink1": "ici",
  "TechnologyIntroductionDescription2": ".",
  "TechnologyIntroductionDescription3": "Pour l'affichage et la manipulation des modèles 3D,",
  "TechnologyIntroductionDescription4": "est utilisé. Pour générer le texte de conversation, divers LLM tels que",
  "TechnologyIntroductionDescription5": "sont utilisés. Pour la synthèse vocale, divers moteurs TTS comme",
  "TechnologyIntroductionDescription6": "sont utilisés. Pour plus de détails, consultez cet",
  "TechnologyIntroductionLink2": "article explicatif",
  "TechnologyIntroductionDescription7": ".",
  "SourceCodeDescription1": "Le code source de cette application est disponible publiquement sur GitHub. N'hésitez pas à le modifier et l'adapter comme vous le souhaitez.",
  "SourceCodeDescription2": "Pour une utilisation commerciale, veuillez consulter le README du même dépôt.",
  "RepositoryURL": "URL du dépôt :",
  "DontShowIntroductionNextTime": "Ne plus afficher cette boîte de dialogue",
  "Close": "FERMER",
  "Contact": "Contact",
  "ContactDescription": "Veuillez me contacter via l'adresse e-mail ou le compte Twitter ci-dessous concernant cette application.",
  "Creator": "Créateur",
  "CreatorDescription": "Créateur : Tegan",
  "Language": "Langue",
  "UsingGSVITTS": "GSVI TTS",
  "GSVITTSInfo": "Paramètres GSVI TTS",
  "GSVITTSServerUrl": "API endpoint GSVI TTS",
  "GSVITTSModelID": "ID du modèle GSVI TTS",
  "GSVITTSBatchSize": "Taille du lot GSVI TTS (1 ~ 100 Plus la valeur est grande, plus la vitesse d'inférence est rapide, mais cela peut épuiser la mémoire si trop grand.)",
  "GSVITTSSpeechRate": "Débit de parole (0.5 ~ 2.0 Plus la valeur est grande, plus c'est rapide.)",
  "UsingElevenLabs": "ElevenLabs",
  "ElevenLabsInfo": "L'API ElevenLabs est utilisée. Elle prend en charge plusieurs langues. La clé API peut être obtenue depuis l'URL ci-dessous.",
  "ElevenLabsApiKey": "Clé API ElevenLabs",
  "ElevenLabsVoiceId": "ID de voix ElevenLabs",
  "ElevenLabsVoiceIdInfo": "L'ID de voix peut être sélectionné depuis l'URL ci-dessous.",
  "CharacterName": "Nom du personnage",
  "ShowAssistantText": "Afficher la boîte de réponse",
  "ShowCharacterName": "Afficher le nom du personnage dans la boîte de réponse",
  "ShowControlPanel": "Afficher le bouton des paramètres",
  "ShowControlPanelInfo": "L'écran des paramètres peut être affiché en appuyant sur Cmd + . (Mac) / Ctrl + . (Windows).",
  "SlideMode": "Mode diaporama",
  "SelectedSlideDocs": "Documents diaporama sélectionnés",
  "SlideModeDescription": "C'est un mode où l'IA présente automatiquement des diapositives. Il n'est disponible que lorsque le service IA sélectionné est OpenAI, Anthropic Claude ou Google Gemini.",
  "PdfConvertLabel": "Conversion de diapositives PDF",
  "PdfConvertDescription": "Convertir PDF en données de mode diaporama. Disponible uniquement lorsque le service IA sélectionné est OpenAI, Anthropic Claude ou Google Gemini.",
  "PdfConvertFileUpload": "Sélectionner le fichier PDF",
  "PdfConvertFolderName": "Nom du dossier de sauvegarde",
  "PdfConvertModelSelect": "Sélectionner le modèle",
  "PdfConvertButton": "Convertir PDF en diapositives",
  "PdfConvertLoading": "Conversion en cours...",
  "PdfConvertSuccess": "Conversion terminée",
  "PdfConvertError": "Échec de la conversion",
  "PdfConvertSubmitError": "Veuillez vous assurer que le fichier PDF, le nom du dossier et la clé API sont définis.",
  "LocalStorageReset": "Réinitialiser les paramètres",
  "LocalStorageResetInfo": "Les variables d'environnement sont prioritaires si définies. La page sera rechargée.",
  "LocalStorageResetButton": "Réinitialiser les paramètres",
  "Errors": {
    "EmptyAPIKey": "La clé API n'est pas définie",
    "AIInvalidProperty": "Les paramètres du service IA sont incorrects",
    "AIAPIError": "Une erreur s'est produite lors de l'exécution de l'API IA",
    "InvalidAIService": "Le service IA sélectionné n'est pas valide",
    "MethodNotAllowed": "La requête n'est pas appropriée",
    "TTSServiceError": "Une erreur s'est produite dans le service TTS {{serviceName}} : {{message}}",
    "UnexpectedError": "Une erreur inattendue s'est produite",
    "LocalLLMError": "Erreur LLM locale",
    "LocalLLMStreamError": "Erreur de flux LLM local",
    "LocalLLMConnectionError": "Erreur de connexion au serveur LLM local",
    "LocalLLMNotFound": "Point de terminaison LLM local non trouvé",
    "LocalLLMAPIError": "Erreur API LLM locale"
  },
  "MessageReceiver": "Recevoir des instructions de l'extérieur",
  "MessageReceiverDescription": "Vous pouvez utiliser l'API pour faire parler les personnages IA depuis l'extérieur.",
  "ClientID": "ID client",
  "OpenSendMessagePage": "Ouvrir la page d'envoi de message",
  "RealtimeAPIMode": "Mode API en temps réel",
  "RealtimeAPIModeContentType": "Type d'envoi",
  "RealtimeAPIModeVoice": "Type de voix",
  "AudioMode": "Mode audio (Beta)",
  "InputText": "Texte",
  "InputAudio": "Audio",
  "SearchGrounding": "Utiliser la recherche contextuelle",
  "SearchGroundingDescription": "Lors de l'utilisation de la fonction multimodale, la fonction de recherche est automatiquement désactivée.",
  "UpdateRealtimeAPISettings": "Mettre à jour les paramètres API en temps réel",
  "UpdateRealtimeAPISettingsInfo": "Lors de la mise à jour de la clé API, du point de terminaison Azure, du type de voix, du modèle ou de l'invite système, veuillez appuyer sur le bouton de mise à jour pour démarrer une nouvelle session WebSocket.",
  "AzureEndpoint": "Point de terminaison Azure",
  "Toasts": {
    "WebSocketConnectionError": "Erreur survenue dans la connexion WebSocket",
    "WebSocketConnectionClosed": "Connexion WebSocket fermée",
    "WebSocketConnectionAttempt": "Tentative de connexion WebSocket...",
    "WebSocketConnectionSuccess": "Connexion WebSocket réussie",
    "FunctionExecuting": "Exécution de {{funcName}}",
    "FunctionExecutionFailed": "L'exécution de {{funcName}} a échoué",
    "FirefoxNotSupported": "Cette fonctionnalité n'est pas prise en charge sur Firefox",
    "SpeechRecognitionError": "Une erreur de reconnaissance vocale s'est produite"
  },
  "UsingOpenAITTS": "Utilisation d'OpenAI",
  "OpenAITTSInfo": "Utilisation d'OpenAI. Prend en charge plusieurs langues. Si vous sélectionnez OpenAI comme service IA, vous n'avez pas besoin de définir la clé API ci-dessous.",
  "OpenAITTSVoice": "Type de voix",
  "OpenAITTSModel": "Modèle",
  "OpenAITTSSpeed": "Vitesse",
  "UsingAzureTTS": "Utilisation d'Azure OpenAI",
  "AzureTTSInfo": "Utilisation d'Azure OpenAI. Prend en charge plusieurs langues.",
  "SendMessage": {
    "title": "Adaptateur externe AITuberKit",
    "directSendTitle": "Parler directement au personnage IA",
    "directSendDescription": "Vous pouvez envoyer le message directement au personnage IA. Si plusieurs messages sont envoyés, ils sont traités dans l'ordre. Le modèle de voix est celui sélectionné dans les paramètres AITuberKit.",
    "aiGenerateTitle": "Générer une réponse IA puis parler",
    "aiGenerateDescription": "L'IA génère une réponse à partir du message envoyé puis la prononce. Si plusieurs messages sont envoyés, ils sont traités dans l'ordre. Le modèle IA et le modèle de voix sont ceux sélectionnés dans les paramètres AITuberKit. L'invite système peut être sélectionnée pour utiliser l'invite système AITuberKit ou une invite système personnalisée. Si vous voulez charger l'historique des conversations passées, incluez la chaîne [conversation_history] dans l'invite système ou le message utilisateur.",
    "useCurrentSystemPrompt": "Utiliser l'invite système AITuberKit",
    "userInputTitle": "Envoyer une entrée utilisateur",
    "userInputDescription": "Le message envoyé est traité de la même manière que lorsqu'il est saisi depuis le formulaire d'entrée AITuberKit. Si plusieurs messages sont envoyés, ils sont traités dans l'ordre. Le modèle IA et le modèle de voix sont ceux sélectionnés dans les paramètres AITuberKit. L'invite système et l'historique des conversations sont les valeurs définies dans AITuberKit."
  },
  "CannotUseVoice": "Le mode API en temps réel ou le mode audio est activé, donc les paramètres de voix ne sont pas nécessaires.",
  "Live2D": {
    "FileInfo": "Placez le modèle Live2D que vous souhaitez utiliser dans le dossier public/live2d. Le fichier model3.json doit exister à la racine de ce dossier.\nS'il n'est pas affiché dans la sélection, veuillez recharger l'écran ou vérifier si le chemin du dossier est correct.",
    "Info": "Vous pouvez spécifier les émotions et les mouvements.\nChaque émotion est contrôlée par l'invite. Pour plus de détails, veuillez consulter \"Paramètres IA => Paramètres de personnage\".",
    "Emotions": "Paramètres d'émotion",
    "EmotionInfo": "Les émotions peuvent être spécifiées au format séparé par des virgules. Si plusieurs émotions sont spécifiées, elles sont sélectionnées aléatoirement.\nLa valeur initiale est pour le modèle fourni par AITuberKit. Si vous utilisez un modèle original, veuillez entrer la valeur selon votre modèle.\nAprès la fin de la conversation, l'émotion \"Neutre\" est affichée.",
    "neutralEmotions": "Neutre",
    "happyEmotions": "Heureux",
    "sadEmotions": "Triste",
    "angryEmotions": "En colère",
    "relaxedEmotions": "Détendu",
    "MotionGroups": "Paramètres des groupes de mouvement",
    "MotionGroupsInfo": "Les groupes de mouvement sont sélectionnés aléatoirement dans le groupe sélectionné.\nComme pour les paramètres d'émotion, veuillez les définir selon votre modèle.\n\"Idle\" est le mouvement affiché après la fin de la conversation.",
    "SelectMotionGroup": "Sélectionner le groupe de mouvement",
    "idleMotionGroup": "Inactif",
    "neutralMotionGroup": "Neutre",
    "happyMotionGroup": "Heureux",
    "sadMotionGroup": "Triste",
    "angryMotionGroup": "En colère",
    "relaxedMotionGroup": "Détendu"
  },
  "UseVideoAsBackground": "Utiliser l'écran partagé ou la webcam comme arrière-plan",
  "Temperature": "Température"
}
