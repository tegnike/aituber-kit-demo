{
  "Description": "■ O aplikacji",
  "BasedSettings": "■ Ustawienia podstawowe",
  "AISettings": "■ Ustawienia AI",
  "CharacterSettings": "■ Ustawienia postaci",
  "YoutubeSettings": "■ Ustawienia YouTube",
  "VoiceSettings": "■ Ustawienia głosu",
  "SlideSettings": "■ Ustawienia slajdów",
  "LogSettings": "■ Historia rozmów",
  "OtherSettings": "■ Inne",
  "ExternalLinkageMode": "Tryb zewnętrznej integracji (WebSocket)",
  "YoutubeMode": "Tryb YouTube",
  "YoutubeInfo": "Komentarze zaczynające się od „#” są ignorowane.",
  "YoutubeAPIKey": "Klucz API YouTube",
  "YoutubeLiveID": "ID YouTube Live",
  "ConversationContinuityMode": "Tryb kontynuacji rozmowy (beta)",
  "ConversationContinuityModeInfo": "Tryb, w którym AI automatycznie kontynuuje rozmowę, gdy brak komentarzy. Obecnie obsługiwane są tylko: OpenAI, Anthropic Claude, Google Gemini.",
  "ConversationContinuityModeInfo2": "Ponieważ w jednej odpowiedzi wywoływane są wielokrotnie LLM, może to zwiększyć koszty API. Proszę uważać.",
  "ConversationContinuityModeInfo3": "Działa stosunkowo stabilnie z modelami gpt-4o, gpt-4-turbo, claude-3-opus, claude-3.5-sonnet.",
  "MaxPastMessages": "Liczba przechowywanych poprzednich wiadomości",
  "StatusOn": "Stan: ON",
  "StatusOff": "Stan: WYŁĄCZONY",
  "Select": "Proszę wybrać",
  "TestVoice": "Przetestuj głos",
  "SelectAIService": "Wybierz usługę AI",
  "LocalLLM": "Lokalny LLM",
  "SelectModel": "Wybierz model",
  "OpenAIAPIKeyLabel": "Klucz API OpenAI",
  "AnthropicAPIKeyLabel": "Klucz API Anthropic",
  "GoogleAPIKeyLabel": "Klucz API Google Gemini",
  "AzureAPIKeyLabel": "Klucz API Azure OpenAI",
  "AzureAPIURL": "URL Azure OpenAI API",
  "GroqAPIKeyLabel": "Klucz API Groq",
  "CohereAPIKeyLabel": "Klucz API Cohere",
  "MistralAIAPIKeyLabel": "Klucz API MistralAI",
  "PerplexityAPIKeyLabel": "Klucz API Perplexity",
  "FireworksAPIKeyLabel": "Klucz API Fireworks",
  "DifyAPIKeyLabel": "Klucz API Dify",
  "DeepSeekAPIKeyLabel": "Klucz API DeepSeek",
  "APIKeyInstruction": "Klucz API można uzyskać za pomocą poniższego linku. Wprowadź uzyskany klucz API do formularza.",
  "LocalLLMInfo": "Należy uruchomić serwer lokalnego LLM.",
  "LocalLLMInfo2": "Wprowadź URL lokalnego LLM (wraz z numerem portu) oraz nazwę modelu.",
  "GroqInfo": "Groq API jest dostępne bezpośrednio przez przeglądarkę.",
  "DifyInfo": "W Dify obsługiwane są tylko chatboty lub typy agentów.",
  "DifyInfo2": "Długość historii rozmów zależy od ustawień chatbota Dify.",
  "DifyInfo3": "Przykład: https://api.dify.ai/v1, http://localhost:80/v1",
  "DifyInstruction": "Jeśli korzystasz z Dify, ten systemowy prompt nie będzie używany. Ustaw go w chatbotie Dify.",
  "EnterURL": "Wprowadź URL",
  "CharacterModelLabel": "Model postaci",
  "CharacterModelInfo": "W zależności od modelu, ładowanie przy pierwszym wyświetleniu może zająć trochę czasu.",
  "OpenVRM": "Otwórz VRM",
  "BackgroundImage": "Obraz tła",
  "ChangeBackgroundImage": "Zmień obraz tła",
  "CharacterSettingsPrompt": "Prompt postaci",
  "CharacterSettingsInfo": "Ta wartość jest ustawiana jako systemowy prompt.\nBazując na początkowym promptcie, możesz określić tagi emocji, aby kontrolować wyraz twarzy i ruchy postaci. Przykład: [neutral] Dzień dobry! [happy] Miłego dnia!",
  "CharacterSettingsReset": "Resetuj ustawienia postaci",
  "SyntheticVoiceEngineChoice": "Wybór silnika syntezatora mowy",
  "VoiceAdjustment": "Dostosowanie głosu",
  "VoiceEngineInstruction": "Wybierz silnik syntezatora mowy, którego chcesz używać.",
  "UsingKoeiromap": "Użyj Koeiromap",
  "KoeiromapInfo": "Używa API Koeiromap firmy Koemotion. Obsługuje tylko język japoński. Szczegóły znajdziesz poniżej.",
  "UsingVoiceVox": "Użyj VOICEVOX",
  "VoiceVoxInfo": "Używa VOICEVOX. Obsługuje tylko język japoński. Ponieważ korzysta z lokalnego API, musisz pobrać aplikację odpowiednią dla Twojego środowiska ze strony poniżej i ją uruchomić.",
  "VoicevoxSpeed": "Szybkość mówienia",
  "VoicevoxPitch": "Wysokość dźwięku",
  "VoicevoxIntonation": "Intonacja",
  "VoicevoxServerUrl": "URL serwera VOICEVOX",
  "UsingAivisSpeech": "Użyj AivisSpeech",
  "AivisSpeechInfo": "Używa AivisSpeech. Obsługuje tylko język japoński. Ponieważ korzysta z lokalnego API, musisz pobrać aplikację odpowiednią dla Twojego środowiska ze strony poniżej i ją uruchomić.",
  "AivisSpeechSpeaker": "Lektor",
  "AivisSpeechSpeed": "Szybkość mówienia",
  "AivisSpeechPitch": "Wysokość dźwięku",
  "AivisSpeechIntonation": "Intonacja",
  "AivisSpeechServerUrl": "URL serwera AivisSpeech",
  "UsingNijiVoice": "Użyj NijiVoice",
  "NijiVoiceInfo": "Używa API NijiVoice. Obsługuje tylko język japoński. Uzyskaj klucz API z poniższego URL.",
  "NijiVoiceApiKey": "Klucz API NijiVoice",
  "NijiVoiceActorId": "ID mówcy",
  "NijiVoiceSpeed": "Szybkość mówienia",
  "NijiVoiceEmotionalLevel": "Poziom emocji",
  "NijiVoiceSoundDuration": "Długość dźwięku",
  "UpdateSpeakerList": "Aktualizuj listę mówców",
  "UsingGoogleTTS": "Użyj Google TTS",
  "UsingStyleBertVITS2": "Użyj Style-Bert-VITS2",
  "StyleBertVITS2Info": "Używa Style-Bert-VITS2. Obsługuje tylko japoński, angielski i chiński. Jeśli korzystasz z lokalnego API, musisz pobrać i uruchomić aplikację odpowiednią dla Twojego środowiska ze strony poniżej. W razie potrzeby ustaw także klucz API.",
  "SpeakerSelection": "Wybór typu głosu",
  "EnglishToJapanese": "Czytaj angielskie słowa po japońsku",
  "IncludeTimestampInUserMessage": "Dołącz znacznik czasu do wypowiedzi użytkownika",
  "IncludeTimestampInUserMessageInfo": "Dołączenie znacznika czasu do wypowiedzi użytkownika umożliwia AI uwzględnienie czasu przy generowaniu odpowiedzi.\nProszę dołączyć następujący tekst do systemowego prompta:\n\n„W niektórych przypadkach wejście użytkownika będzie zawierać [timestamp]. Oznacza to, że podany czas reprezentuje czas UTC w momencie żądania, więc wygeneruj odpowiedź, uwzględniając ten czas.”",
  "GoogleTTSInfo": "Używa Google Cloud Text-to-Speech. Obsługuje wiele języków.",
  "AuthFileInstruction": "Wymagany jest klucz API lub plik JSON do autoryzacji. Pobierz go z poniższego linku i, w przypadku pliku JSON, umieść go w katalogu głównym repozytorium jako credentials.json.",
  "LanguageModelURL": "Wybierz model językowy z poniższego URL.",
  "LanguageChoice": "Wybór języka",
  "StyleBeatVITS2ServerURL": "URL serwera",
  "StyleBeatVITS2ApiKey": "Klucz API",
  "StyleBeatVITS2ModelID": "ID modelu",
  "StyleBeatVITS2Style": "Styl",
  "StyleBeatVITS2SdpRatio": "Stosunek SDP/DP",
  "StyleBeatVITS2Length": "Prędkość mówienia",
  "ConversationHistory": "Historia rozmów",
  "ConversationHistoryInfo": "Ostatnie 10 wiadomości z rozmowy jest zapamiętywanych.",
  "ConversationHistoryReset": "Resetuj historię rozmów",
  "NotConnectedToExternalAssistant": "Nie połączono z zewnętrznym asystentem.",
  "APIKeyNotEntered": "Klucz API nie został wprowadzony.",
  "ChatLog": "Log rozmów",
  "EnterYourQuestion": "Wpisz swoje pytanie",
  "AnswerGenerating": "Generowanie odpowiedzi",
  "AboutThisApplication": "O tej aplikacji",
  "AboutThisApplicationDescription": "Dzięki tej aplikacji możesz rozmawiać z 3D postacią wyłącznie za pomocą przeglądarki internetowej, korzystając z mikrofonu, wprowadzania tekstu i syntezy mowy. Możesz również zmienić postać (VRM), ustawić jej osobowość oraz dostosować głos. <br /> Ustawienia można zmienić poprzez przycisk menu w lewym górnym rogu.",
  "AboutThisApplicationDescription2": "AITuberKit umożliwia rozmowę z postacią AI wyłącznie za pomocą przeglądarki. Aby zmienić postać, ustawić jej osobowość lub dostosować głos, sprawdź odpowiednie ustawienia.",
  "TechnologyIntroduction": "Wprowadzenie do technologii",
  "TechnologyIntroductionDescription1": "Ta aplikacja została stworzona na podstawie zmodyfikowanej wersji <b>ChatVRM</b> firmy pixiv. Oryginalny kod źródłowy jest dostępny",
  "TechnologyIntroductionLink1": "tutaj",
  "TechnologyIntroductionDescription2": "proszę zapoznać się z nim.",
  "TechnologyIntroductionDescription3": "Do wyświetlania i obsługi modeli 3D używane są",
  "TechnologyIntroductionDescription4": ", a do generowania treści rozmów używane są",
  "TechnologyIntroductionDescription5": "różne modele LLM, a do syntezy mowy",
  "TechnologyIntroductionDescription6": "wykorzystuje się różne systemy TTS. Szczegóły znajdziesz w",
  "TechnologyIntroductionLink2": "artykule wyjaśniającym",
  "TechnologyIntroductionDescription7": "zapoznaj się z nim.",
  "SourceCodeDescription1": "Kod źródłowy tej aplikacji jest udostępniony na GitHubie. Można go dowolnie modyfikować i zmieniać.",
  "SourceCodeDescription2": "W sprawach komercyjnego wykorzystania, proszę zapoznać się z README w tym repozytorium.",
  "RepositoryURL": "URL repozytorium:",
  "DontShowIntroductionNextTime": "Nie pokazuj tego dialogu przy następnym uruchomieniu",
  "Close": "Zamknij",
  "Contact": "Kontakt",
  "ContactDescription": "W przypadku pytań dotyczących tej aplikacji proszę kontaktować się za pomocą poniższego adresu e-mail lub konta na Twitterze.",
  "Creator": "Informacje o twórcy",
  "CreatorDescription": "Twórca: Nike",
  "Language": "Ustawienia języka",
  "UsingGSVITTS": "Użyj GSVI TTS",
  "GSVITTSInfo": "Ustawienia GSVI TTS",
  "GSVITTSServerUrl": "URL serwera GSVI TTS",
  "GSVITTSModelID": "ID modelu GSVI TTS",
  "GSVITTSBatchSize": "Rozmiar partii GSVI TTS (1 ~ 100; większa wartość przyspiesza inferencję, ale zbyt duża może spowodować wyczerpanie pamięci)",
  "GSVITTSSpeechRate": "Szybkość mówienia (0.5 ~ 2.0; większa wartość oznacza szybsze tempo)",
  "UsingElevenLabs": "Użyj ElevenLabs",
  "ElevenLabsInfo": "Używa API ElevenLabs. Obsługuje wiele języków. Uzyskaj klucz API z poniższego URL.",
  "ElevenLabsApiKey": "Klucz API ElevenLabs",
  "ElevenLabsVoiceId": "ID głosu ElevenLabs",
  "ElevenLabsVoiceIdInfo": "Wybierz ID głosu z poniższego URL.",
  "CharacterName": "Nazwa postaci",
  "ShowAssistantText": "Pokaż pole odpowiedzi",
  "ShowCharacterName": "Pokaż nazwę postaci w polu odpowiedzi",
  "ShowControlPanel": "Pokaż panel sterowania",
  "ShowControlPanelInfo": "Panel ustawień można wywołać skrótem Cmd + . (Mac) / Ctrl + . (Windows).",
  "SlideMode": "Tryb slajdów",
  "SelectedSlideDocs": "Wybrane slajdy",
  "SlideModeDescription": "Tryb, w którym AI automatycznie prezentuje slajdy. Aktywny tylko, gdy wybrana usługa AI to OpenAI, Anthropic Claude lub Google Gemini.",
  "PdfConvertLabel": "Konwersja PDF do slajdów",
  "PdfConvertDescription": "Konwertuje PDF do danych używanych w trybie slajdów. Dostępne tylko, gdy wybrana usługa AI to OpenAI, Anthropic Claude lub Google Gemini.",
  "PdfConvertFileUpload": "Wybierz plik PDF",
  "PdfConvertFolderName": "Nazwa folderu do zapisu",
  "PdfConvertModelSelect": "Wybierz model",
  "PdfConvertButton": "Konwertuj PDF do slajdów",
  "PdfConvertLoading": "Konwertowanie...",
  "PdfConvertSuccess": "Konwersja zakończona pomyślnie",
  "PdfConvertError": "Konwersja nie powiodła się",
  "PdfConvertSubmitError": "Sprawdź, czy plik PDF, nazwa folderu i klucz API zostały ustawione.",
  "LocalStorageReset": "Resetuj ustawienia",
  "LocalStorageResetInfo": "Jeśli zmienne środowiskowe są ustawione, mają pierwszeństwo. Strona zostanie przeładowana.",
  "LocalStorageResetButton": "Resetuj ustawienia",
  "Errors": {
    "EmptyAPIKey": "Nie ustawiono klucza API",
    "AIInvalidProperty": "Nieprawidłowe wartości ustawień usługi AI",
    "AIAPIError": "Wystąpił błąd podczas wykonywania API AI",
    "InvalidAIService": "Wybrana usługa AI jest nieprawidłowa",
    "MethodNotAllowed": "Żądanie jest nieprawidłowe",
    "TTSServiceError": "W usłudze TTS {{serviceName}} wystąpił błąd: {{message}}",
    "UnexpectedError": "Wystąpił nieznany błąd",
    "LocalLLMError": "Wystąpił błąd w lokalnym LLM",
    "LocalLLMStreamError": "Wystąpił błąd podczas przetwarzania strumienia lokalnego LLM",
    "LocalLLMConnectionError": "Nie można połączyć się z serwerem lokalnego LLM",
    "LocalLLMNotFound": "Nie znaleziono punktu końcowego lokalnego LLM",
    "LocalLLMAPIError": "Wystąpił błąd w API lokalnego LLM"
  },
  "MessageReceiver": "Przyjmowanie poleceń z zewnątrz",
  "MessageReceiverDescription": "Za pomocą API można sterować wypowiedziami postaci AI z zewnątrz.",
  "ClientID": "ID klienta",
  "OpenSendMessagePage": "Otwórz stronę wysyłania wiadomości",
  "RealtimeAPIMode": "Tryb API w czasie rzeczywistym",
  "RealtimeAPIModeContentType": "Typ wysyłki",
  "RealtimeAPIModeVoice": "Typ głosu",
  "AudioMode": "Tryb audio (beta)",
  "InputText": "Tekst",
  "InputAudio": "Audio",
  "SearchGrounding": "Użyj funkcji wyszukiwania",
  "SearchGroundingDescription": "Gdy korzystasz z funkcji multimodalnych, funkcja wyszukiwania zostaje automatycznie wyłączona.",
  "UpdateRealtimeAPISettings": "Aktualizuj ustawienia API w czasie rzeczywistym",
  "UpdateRealtimeAPISettingsInfo": "Po zaktualizowaniu klucza API, Azure Endpoint, typu głosu, modelu lub systemowego prompta, naciśnij przycisk aktualizacji, aby rozpocząć nową sesję WebSocket.",
  "AzureEndpoint": "Azure Endpoint",
  "Toasts": {
    "WebSocketConnectionError": "Wystąpił błąd połączenia WebSocket.",
    "WebSocketConnectionClosed": "Połączenie WebSocket zostało zamknięte",
    "WebSocketConnectionAttempt": "Próba połączenia WebSocket...",
    "WebSocketConnectionSuccess": "Połączenie WebSocket zakończone sukcesem.",
    "FunctionExecuting": "Wykonywanie {{funcName}}.",
    "FunctionExecutionFailed": "Wykonanie {{funcName}} nie powiodło się.",
    "FirefoxNotSupported": "Ta funkcja nie jest obsługiwana w Firefoxie",
    "SpeechRecognitionError": "Wystąpił błąd rozpoznawania mowy"
  },
  "UsingOpenAITTS": "Użyj OpenAI",
  "OpenAITTSInfo": "Używa OpenAI. Obsługuje wiele języków. Jeśli wybrano OpenAI jako usługę AI, nie ma potrzeby ustawiania klucza API poniżej.",
  "OpenAITTSVoice": "Typ głosu",
  "OpenAITTSModel": "Model",
  "OpenAITTSSpeed": "Szybkość mówienia",
  "UsingAzureTTS": "Użyj Azure OpenAI",
  "AzureTTSInfo": "Używa Azure OpenAI. Obsługuje wiele języków.",
  "SendMessage": {
    "title": "Adapter zewnętrzny AITuberKit",
    "directSendTitle": "Pozwól postaci AI mówić bezpośrednio",
    "directSendDescription": "Wysłane wiadomości zostaną bezpośrednio wypowiedziane przez postać AI. W przypadku wielu wiadomości będą przetwarzane kolejno.\nUżywany jest model głosu wybrany w ustawieniach AITuberKit.",
    "aiGenerateTitle": "Pozwól AI wygenerować odpowiedź, a następnie wypowiedzieć ją",
    "aiGenerateDescription": "AI wygeneruje odpowiedź na podstawie wysłanej wiadomości, a następnie postać AI ją wypowie. W przypadku wielu wiadomości będą przetwarzane kolejno.\nUżywane są modele AI i głosu wybrane w ustawieniach AITuberKit.\nMożesz wybrać, czy używać systemowego prompta AITuberKit, czy niestandardowego prompta.\nJeśli chcesz załadować historię rozmów, dołącz do systemowego prompta lub dowolnej pozycji wiadomości użytkownika ciąg znaków [conversation_history].",
    "useCurrentSystemPrompt": "Użyj systemowego prompta AITuberKit",
    "userInputTitle": "Wyślij dane wejściowe użytkownika",
    "userInputDescription": "Wysłane wiadomości są przetwarzane tak samo, jak gdyby zostały wprowadzone w formularzu AITuberKit. W przypadku wielu wiadomości będą przetwarzane kolejno.\nUżywane są modele AI i głosu wybrane w ustawieniach AITuberKit.\nSystemowy prompt oraz historia rozmów są ustawiane zgodnie z wartościami AITuberKit."
  },
  "CannotUseVoice": "Gdy tryb API w czasie rzeczywistym lub tryb audio jest aktywny,\nustawienia głosu nie są potrzebne.",
  "Live2D": {
    "FileInfo": "Umieść folder z modelem Live2D, którego chcesz użyć, w katalogu public/live2d. W katalogu tym musi znajdować się plik model3.json.\nJeśli nie pojawi się w opcjach, odśwież stronę lub upewnij się, że ścieżka do folderu jest poprawna.",
    "Info": "Możesz określić emocje i ruchy.\nKażda emocja jest kontrolowana przez prompt. Szczegóły znajdziesz w „Ustawieniach AI => Ustawienia postaci”.",
    "Emotions": "Ustawienia wyrazu twarzy",
    "EmotionInfo": "Emocje można określić jako wiele, oddzielonych przecinkami. Jeśli podasz wiele, będą wybierane losowo.\nWartości początkowe odpowiadają modelom dostarczonym przez AITuberKit. Jeśli używasz własnego modelu, wprowadź odpowiednie wartości.\nPo zakończeniu rozmowy wyświetlany będzie wyraz „normalny”.",
    "neutralEmotions": "Normalny",
    "happyEmotions": "Szczęśliwy",
    "sadEmotions": "Smutny",
    "angryEmotions": "Zły",
    "relaxedEmotions": "Zrelaksowany",
    "MotionGroups": "Ustawienia grup ruchów",
    "MotionGroupsInfo": "Grupy ruchów będą wybierane losowo z wybranej grupy.\nPodobnie jak ustawienia wyrazu twarzy, dostosuj je do swojego modelu.\n„Podczas bezczynności” to ruch wyświetlany po zakończeniu rozmowy.",
    "SelectMotionGroup": "Wybierz grupę ruchów",
    "idleMotionGroup": "Podczas bezczynności",
    "neutralMotionGroup": "Normalny",
    "happyMotionGroup": "Szczęśliwy",
    "sadMotionGroup": "Smutny",
    "angryMotionGroup": "Zły",
    "relaxedMotionGroup": "Zrelaksowany"
  },
  "UseVideoAsBackground": "Użyj ekranu udostępniania lub kamery internetowej jako tła",
  "Temperature": "Temperatura"
}
